{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edenshmuel/PapaJohns_Data_Science_Project/blob/main/TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "N1it5HhCGKcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 1. Install & import libraries\n",
        "!pip install -q pytorch-lightning torchmetrics pytorch-forecasting"
      ],
      "metadata": {
        "id": "ret2HfrYJIKh",
        "outputId": "8718480d-aea4-4cf2-856a-af584ceb0abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/823.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m819.2/823.0 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.7/197.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import (\n",
        "    TimeSeriesDataSet, TemporalFusionTransformer, Baseline,\n",
        "    QuantileLoss)"
      ],
      "metadata": {
        "id": "1X7EyT3lF6ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hi4pZl53GPD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Final_Project_PapaJohns/cleaned_data.csv')"
      ],
      "metadata": {
        "id": "Y8-j9ZDPLoPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc_map = pd.read_csv('/content/drive/MyDrive/Final_Project_PapaJohns/desc_encoding_map.csv')\n",
        "cat_map = pd.read_csv('/content/drive/MyDrive/Final_Project_PapaJohns/category_mapping.csv')"
      ],
      "metadata": {
        "id": "jbXcSLMvLoKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])"
      ],
      "metadata": {
        "id": "xJZSAN49GkgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={\"כמות\": \"quantity\"})"
      ],
      "metadata": {
        "id": "lwysNadsGls0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 3. Build a numeric time index (required by PyTorch‑Forecasting)\n",
        "df = df.sort_values([\"clean_desc_encoded\", \"Date\"])\n",
        "df[\"time_idx\"] = (\n",
        "    df.groupby(\"clean_desc_encoded\")\n",
        "      .cumcount())"
      ],
      "metadata": {
        "id": "cCRW_6KpGspl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 4. Define TFT parameters\n",
        "max_encoder_length     = 60      # how many historic days the model sees\n",
        "max_prediction_length  = 30      # horizon (1 week – 1 month, tweak as you like)"
      ],
      "metadata": {
        "id": "la0vv7FLGlkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_cutoff = df[\"time_idx\"].max() - max_prediction_length"
      ],
      "metadata": {
        "id": "zzWWE79vGyOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 5. Tell the dataset which columns play which role\n",
        "categorical_static = [\"clean_desc_encoded\", \"category_encoded\",\n",
        "                      \"encoded_portion_type\"]\n",
        "categorical_time   = [\"Is_Weekend\", \"Season\",\n",
        "                      \"is_christian_holiday\", \"is_jewish_holiday\",\n",
        "                      \"is_near_jewish_holiday\", \"is_day_before_new_year\",\n",
        "                      \"encoded_jewish_holiday\", \"encoded_christian_holiday\"]"
      ],
      "metadata": {
        "id": "Rj91wD_dGlcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# המרה לכל העמודות הקטגוריאליות ל-string ואז ל-category כדי שהמודל יבין שאלו קטגוריות\n",
        "for col in categorical_static + categorical_time:\n",
        "    df[col] = df[col].astype(str).astype(\"category\")"
      ],
      "metadata": {
        "id": "lk-gABpFH2vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All the real‑valued, time‑varying features\n",
        "real_time = [\n",
        "    \"Year\", \"Month\", \"Day\", \"WeekOfYear\",\n",
        "    \"Day_Name_sin\", \"Day_Name_cos\", \"Month_sin\", \"Month_cos\",\n",
        "    \"avg_quantity_all_time\", \"std_quantity_all_time\",\n",
        "    \"num_days_sold\", \"popularity_score\"]"
      ],
      "metadata": {
        "id": "7JKFNVQHG3hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 6. Wrap everything in a TimeSeriesDataSet\n",
        "training = TimeSeriesDataSet(\n",
        "    df[df.time_idx <= training_cutoff],\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"quantity\",\n",
        "    group_ids=[\"clean_desc_encoded\"],\n",
        "\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "\n",
        "    static_categoricals=categorical_static,\n",
        "    time_varying_known_categoricals=categorical_time,\n",
        "    time_varying_known_reals=real_time,\n",
        "    time_varying_unknown_reals=[\"quantity\"],  # the target itself\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,)\n",
        "\n",
        "validation = TimeSeriesDataSet.from_dataset(\n",
        "    training, df, min_prediction_idx=training_cutoff+1)"
      ],
      "metadata": {
        "id": "xo_oW_IJG3Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 7. Dataloaders\n",
        "batch_size = 128\n",
        "train_loader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
        "val_loader   = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)"
      ],
      "metadata": {
        "id": "U1b8bb3hHGKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 8. Define the model\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate      = 1e-3,\n",
        "    hidden_size        = 32,\n",
        "    attention_head_size= 4,\n",
        "    dropout            = 0.1,\n",
        "    loss               = QuantileLoss(),\n",
        "    log_interval       = 10,\n",
        "    reduce_on_plateau_patience = 4,)"
      ],
      "metadata": {
        "id": "TFFL1rTXHGCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning.pytorch as pl\n",
        "\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor"
      ],
      "metadata": {
        "id": "BSkYXNO1MyG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 9. Train\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")])\n",
        "\n",
        "trainer.fit(tft, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "sakLZuh9HBdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 10. Forecast next 30 days for all items\n",
        "raw_predictions, x, index = tft.predict(val_loader, mode=\"raw\", return_x=True)"
      ],
      "metadata": {
        "id": "9FrCk4aDHNbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 11. Evaluate\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "y_true = np.concatenate([y[0].numpy() for y in x[\"decoder_target\"]])\n",
        "y_pred = np.concatenate([p[0].numpy() for p in raw_predictions[\"prediction\"]])\n",
        "\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae:.2f} | RMSE: {rmse:.2f} | R²: {r2:.3f}\")"
      ],
      "metadata": {
        "id": "42VXhqSnHNVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇️ 12. Plot an example item\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "item_id = df[\"clean_desc_encoded\"].sample(1).iloc[0]\n",
        "tft.plot_prediction(x, raw_predictions, idx=item_id)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9yB-iFPTHNQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDoow8jqHNNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5RFgq4XHNJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vxldH4pTHBXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# עותק מהנתונים המקוריים\n",
        "df = cleaned_data.copy()"
      ],
      "metadata": {
        "id": "PLDUkjmKLoIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_forecasting import TimeSeriesDataSet\n",
        "import pandas as pd\n",
        "\n",
        "def build_tft_datasets_from_raw(\n",
        "    df,\n",
        "    desc_map,\n",
        "    cat_map,\n",
        "    target_col=\"כמות\",\n",
        "    time_idx_col=\"order\",\n",
        "    group_col=\"clean_desc_str\",\n",
        "    date_col=\"Date\",\n",
        "    cutoff_date=\"2024-01-01\",\n",
        "    encoder_length=30,\n",
        "    prediction_length=7,\n",
        "):\n",
        "    # שלב 1 – מיזוג מיפויים\n",
        "    desc_map = desc_map.rename(columns={\n",
        "        \"code\": \"clean_desc_encoded\",\n",
        "        \"Unnamed: 0\": \"clean_desc_str\"\n",
        "    })\n",
        "    cat_map = cat_map.rename(columns={\n",
        "        \"קוד\": \"category_encoded\",\n",
        "        \"קטגוריה\": \"category_str\"\n",
        "    })\n",
        "    df = df.merge(desc_map, on=\"clean_desc_encoded\", how=\"left\")\n",
        "    df = df.merge(cat_map, on=\"category_encoded\", how=\"left\")\n",
        "\n",
        "    # שלב 2 – ניקוי והמרת טיפוסים\n",
        "    df[date_col]           = pd.to_datetime(df[date_col])\n",
        "    df[\"clean_desc_str\"]   = df[\"clean_desc_str\"].astype(str).fillna(\"Unknown\")\n",
        "    df[\"category_str\"]     = df[\"category_str\"].astype(str).fillna(\"Unknown\")\n",
        "    df[\"portion_type\"]     = df[\"portion_type\"].astype(str).fillna(\"Unknown\")\n",
        "    df[target_col]         = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
        "    df[\"time_idx\"]         = df[time_idx_col]\n",
        "\n",
        "    # **_drop all rows with the problematic code 54 right here**\n",
        "    df = df[df[\"clean_desc_encoded\"] != 54].copy()\n",
        "\n",
        "    # שלב 3 – פיצול דאטה\n",
        "    train_df = df[df[date_col] < cutoff_date].copy()\n",
        "    val_df   = df[df[date_col] >= cutoff_date].copy()\n",
        "\n",
        "    # שלב 4 – יצירת TimeSeriesDataSet לאימון\n",
        "    known_reals         = [\"time_idx\",\"Month\",\"Day\",\"Day_Name\",\"Is_Weekend\",\n",
        "                           \"is_christian_holiday\",\"is_jewish_holiday\",\"is_near_jewish_holiday\",\n",
        "                           \"is_day_before_new_year\",\"Season\",\"is_start_of_month\",\"is_end_of_month\",\n",
        "                           \"Day_Name_sin\",\"Day_Name_cos\",\"Month_sin\",\"Month_cos\"]\n",
        "    known_categoricals  = [\"portion_type\"]\n",
        "    static_categoricals = [group_col, \"category_str\"]\n",
        "    observed_reals      = [target_col,\"avg_quantity_all_time\",\"std_quantity_all_time\",\n",
        "                           \"popularity_score\",\"num_days_sold\"]\n",
        "\n",
        "    training = TimeSeriesDataSet(\n",
        "        train_df,\n",
        "        time_idx=\"time_idx\",\n",
        "        target=target_col,\n",
        "        group_ids=[group_col],\n",
        "        max_encoder_length=encoder_length,\n",
        "        max_prediction_length=prediction_length,\n",
        "        static_categoricals=static_categoricals,\n",
        "        time_varying_known_reals=known_reals,\n",
        "        time_varying_known_categoricals=known_categoricals,\n",
        "        time_varying_unknown_reals=[target_col] + observed_reals,\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "        allow_missing_timesteps=True,\n",
        "    )\n",
        "\n",
        "    # ——— הקטע החדש: דיבוג ערכים לא מוכרים ב-val_df ———\n",
        "    categorical_encoders = training.get_parameters()[\"categorical_encoders\"]\n",
        "    for col, encoder in categorical_encoders.items():\n",
        "        if col in val_df.columns:\n",
        "            val_values = set(val_df[col].unique())\n",
        "            known      = set(encoder.classes_)\n",
        "            unknown    = val_values - known\n",
        "            if unknown:\n",
        "                print(f\"⚠️ עמודה '{col}' מכילה ערכים לא מוכרים: {unknown}\")\n",
        "    # ——————————————————————————————————————————————\n",
        "\n",
        "    # שלב 5 – סינון ערכים לא חוקיים לפי encoders של המודל\n",
        "    before = len(val_df)\n",
        "    for col, encoder in categorical_encoders.items():\n",
        "        if col in val_df.columns:\n",
        "            legal = encoder.classes_\n",
        "            val_df = val_df[val_df[col].isin(legal)]\n",
        "    after = len(val_df)\n",
        "    print(f\"סוננו {before - after} שורות מ-val_df עם ערכים לא חוקיים בעמודות שקודדו.\")\n",
        "\n",
        "    # שלב 6 – טיפול ידני ב-clean_desc_encoded == 54 אם נשאר\n",
        "    if 54 in val_df.get(\"clean_desc_encoded\", []):\n",
        "        val_df = val_df[val_df[\"clean_desc_encoded\"] != 54]\n",
        "        print(\"⚠️ הסרנו ידנית את הקוד 54 מ-val_df (clean_desc_encoded).\")\n",
        "\n",
        "    # שלב 7 – יצירת validator dataset\n",
        "    validation = TimeSeriesDataSet.from_dataset(training, val_df)\n",
        "\n",
        "    return training, validation"
      ],
      "metadata": {
        "id": "fS3lgoBVmSWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training, validation = build_tft_datasets_from_raw(\n",
        "    df,\n",
        "    desc_map,\n",
        "    cat_map,\n",
        "    cutoff_date=\"2024-01-01\",\n",
        "    encoder_length=30,\n",
        "    prediction_length=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "HEZNg8atmSSC",
        "outputId": "7b0cab65-d7b4-42c3-93ac-47f8d9bbee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py:951: UserWarning: Target scales will be only added for continous targets\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 80 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__clean_desc_str': '10 קלאסיות'}, {'__group_id__clean_desc_str': '2 פיצות ללא גלוטן משפחתיות'}, {'__group_id__clean_desc_str': '2 פיצות משפחתיות קלאסיות rl'}, {'__group_id__clean_desc_str': '3 משולשי קרטון לפיצה'}, {'__group_id__clean_desc_str': '3 פלפלים פפרוציני'}, {'__group_id__clean_desc_str': '4 פיצות 14'}, {'__group_id__clean_desc_str': '5 כדורי פירה'}, {'__group_id__clean_desc_str': '6 גבינות איטסיין'}, {'__group_id__clean_desc_str': '6 פיצות מרגריטה'}, {'__group_id__clean_desc_str': 'אנשובי'}]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ עמודה 'clean_desc_str' מכילה ערכים לא מוכרים: {'נקניקייה טבעונית', 'פיצה l קלאסית תוספת צמחונית משלוח po', '2 משפחתיות קלאסיות', '3 פיצות ללא גלוטן משפחתיות', '6 גבינות 14 דקה', 'עם אורגנו', 'קלפי ליגת האלופות', 'גבינת עיזים', '3 תוספות חינם', 'עם תבלין איטלקי', '6 גבינות 14', 'רולס פירות יער שמנת', 'עם רוטב פיסטוק', 'קרם שוקולד נוגט', 'רוטבים', 'רולס בייגלה שמנת ועוגיות', 'גרליק בייטס 24 יחידות', 'מונסטר אולטרה', 'פיצה מיוחדת ממהדורת חורף פיצה קלאסית', 'רולס מרשמלו שמנת ועוגיות', 'אצבעות פסטו קרונפלקס', 'קורנפלקס דליס', '30 קינוח', 'הבלקנית 14', 'עם רוטב בייגלה', 'רולס פצפוצי רושה ופיסטוק שמנת', 'עם רוטב מרשמלו', '5 גבינות', 'פיצה מיוחדת ממהדורת חורף 2 פחיות או מנת נלוות', '8 טבעות גאודה מצופות', 'קלאסית 12', 'ציפס', 'מונסטר מנגו לוקו', 'אצבעות גבינה פסח', 'עם רוטב פירות יער', 'קרונפלקס פטריות 14', 'פיצה l קלאסית מנה נלוות קינוח', '2 יח עוגיות מדלן שוקולדציפס', 'פיצה משפחתית קלאסית באיסוף בנק מזרחי', 'משפחתית קלאסית 2 פחיות', 'שקית לקוח', 'פיצה משפחתית מנה נלווית שתיה גדולה'}\n",
            "סוננו 15244 שורות מ-val_df עם ערכים לא חוקיים בעמודות שקודדו.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Unknown category '54' encountered. Set `add_nan=True` to allow unknown categories\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y, return_norm, target_scale, ignore_na)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/encoders.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 54",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e589c4b7c018>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training, validation = build_tft_datasets_from_raw(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdesc_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcat_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcutoff_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2024-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bd632bffea3a>\u001b[0m in \u001b[0;36mbuild_tft_datasets_from_raw\u001b[0;34m(df, desc_map, cat_map, target_col, time_idx_col, group_col, date_col, cutoff_date, encoder_length, prediction_length)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# שלב 7 – יצירת validator dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m             \u001b[0mnew\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \"\"\"\n\u001b[0;32m-> 1647\u001b[0;31m         return cls.from_parameters(\n\u001b[0m\u001b[1;32m   1648\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mfrom_parameters\u001b[0;34m(cls, parameters, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1707\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Target normalizer is separate and not in scalers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_normalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNaNLabelEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# overwrite target because it requires encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0;31m# (continuous targets should not be normalized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y, return_norm, target_scale, ignore_na)\u001b[0m\n\u001b[1;32m    416\u001b[0m                     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     raise KeyError(\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;34mf\"Unknown category '{e.args[0]}' encountered. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                         \u001b[0;34m\"Set `add_nan=True` to allow unknown categories\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unknown category '54' encountered. Set `add_nan=True` to allow unknown categories\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# debug: בדוק איפה בדיוק 54 חומק\n",
        "categorical_encoders = training.get_parameters()[\"categorical_encoders\"]\n",
        "for col, encoder in categorical_encoders.items():\n",
        "    # רק אם העמודה הזו באמת קיימת ב־val_df\n",
        "    if col in val_df.columns:\n",
        "        val_values = set(val_df[col].unique())\n",
        "        known = set(encoder.classes_)\n",
        "        unknown = val_values - known\n",
        "        if unknown:\n",
        "            print(f\"⚠️ עמודה '{col}' מכילה ערכים לא מוכרים: {unknown}\")\n"
      ],
      "metadata": {
        "id": "1NkRwhdSmSMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yV7uyTKdluBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQiEejy9lt-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzRlt9Aplt8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbp8hzX_lt6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7Rt_sHblt3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RdTynbS8lt1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnh23njBltyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}